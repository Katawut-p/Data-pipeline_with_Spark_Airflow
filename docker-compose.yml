version: "3.7"
services:
  # postgres used by airflow
  postgres:
    image: postgres:13
    container_name: pipeline_postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      # Create Test database on Postgresql
      - ./postgres-init:/docker-entrypoint-initdb.d
      # Save data in postgres
      - postgres-db-volume:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  webserver:
    image: airflow-spark:2.2.1_3.1.2
    container_name: airflow_webserver
    command: webserver
    depends_on:
      - postgres
      - scheduler
    volumes:
      - ./airflow/dags:/opt/airflow/airflow/dags # Dags folder
      - ./airflow/logs:/opt/airflow/logs # Logs folder
      - ./airflow/plugins:/opt/airflow/plugins # Plugins folder
      - ./airflow/scripts:/opt/airflow/scripts # init-script folder
      - ./spark/app:/usr/local/spark/app # Spark Scripts (Must be the same path in airflow and Spark Cluster)
      - ./spark/resources:/usr/local/spark/resources # Resources folder (Must be the same path in airflow and Spark Cluster)
      - ./postgres_jar:/usr/postgres_jar/
    env_file:
      - .env
    user: "${AIRFLOW_UID:-50000}:0"
    ports:
      - "8080:8080"
      
  scheduler:
    image: airflow-spark:2.2.1_3.1.2
    container_name: airflow_scheduler
    command: scheduler
    depends_on:
      - postgres
    volumes:
      - ./airflow/dags:/opt/airflow/dags # Dags folder
      - ./airflow/logs:/opt/airflow/logs # Logs folder
      - ./airflow/plugins:/opt/airflow/plugins # Plugins folder
      - ./spark/app:/usr/local/spark/app # Spark Scripts (Must be the same path in airflow and Spark Cluster)
      - ./spark/resources:/usr/local/spark/resources # Resources folder (Must be the same path in airflow and Spark Cluster)
      - ./postgres_jar:/usr/postgres_jar/
    env_file:
      - .env
    environment:
      - _AIRFLOW_DB_UPGRADE='true'
      - _AIRFLOW_WWW_USER_CREATE='true'
      - _AIRFLOW_WWW_USER_USERNAME=admin 
      - _AIRFLOW_WWW_USER_PASSWORD=admin
    user: "${AIRFLOW_UID:-50000}:0"

  # Spark with 1 workers
  spark:
    image: bitnami/spark:3.1.2
    container_name: apache_spark
    user: root
    hostname: spark
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
      - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
    ports:
      - "8181:8080"
      - "7077:7077"

  spark-worker-1:
    image: bitnami/spark:3.1.2
    container_name: apache_spark-worker-1
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
      - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)

volumes:
  postgres-db-volume: